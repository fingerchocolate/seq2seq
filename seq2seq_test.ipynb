{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "from nltk.translate import bleu_score\n",
    "#import nltk\n",
    "import numpy\n",
    "import progressbar\n",
    "import six\n",
    "\n",
    "import chainer\n",
    "from chainer.backends import cuda\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import training\n",
    "from chainer.training import extensions\n",
    "\n",
    "\n",
    "UNK = 0\n",
    "EOS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sequence_embed(embed, xs):\n",
    "    x_len = [len(x) for x in xs]\n",
    "    x_section = numpy.cumsum(x_len[:-1])\n",
    "    ex = embed(F.concat(xs, axis=0))\n",
    "    exs = F.split_axis(ex, x_section, 0)\n",
    "    return exs\n",
    "\n",
    "\n",
    "class Seq2seq(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_layers, n_source_vocab, n_target_vocab, n_units):\n",
    "        super(Seq2seq, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.embed_x = L.EmbedID(n_source_vocab, n_units)\n",
    "            self.embed_y = L.EmbedID(n_target_vocab, n_units)\n",
    "            self.encoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.decoder = L.NStepLSTM(n_layers, n_units, n_units, 0.1)\n",
    "            self.W = L.Linear(n_units, n_target_vocab)\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_units = n_units\n",
    "\n",
    "    def forward(self, xs, ys):\n",
    "        xs = [x[::-1] for x in xs]\n",
    "\n",
    "        eos = self.xp.array([EOS], numpy.int32)\n",
    "        ys_in = [F.concat([eos, y], axis=0) for y in ys]\n",
    "        ys_out = [F.concat([y, eos], axis=0) for y in ys]\n",
    "\n",
    "        # Both xs and ys_in are lists of arrays.\n",
    "        exs = sequence_embed(self.embed_x, xs)\n",
    "        eys = sequence_embed(self.embed_y, ys_in)\n",
    "\n",
    "        batch = len(xs)\n",
    "        # None represents a zero vector in an encoder.\n",
    "        hx, cx, _ = self.encoder(None, None, exs)\n",
    "        _, _, os = self.decoder(hx, cx, eys)\n",
    "\n",
    "        # It is faster to concatenate data before calculating loss\n",
    "        # because only one matrix multiplication is called.\n",
    "        concat_os = F.concat(os, axis=0)\n",
    "        concat_ys_out = F.concat(ys_out, axis=0)\n",
    "        loss = F.sum(F.softmax_cross_entropy(\n",
    "            self.W(concat_os), concat_ys_out, reduce='no')) / batch\n",
    "\n",
    "        chainer.report({'loss': loss}, self)\n",
    "        n_words = concat_ys_out.shape[0]\n",
    "        perp = self.xp.exp(loss.array * batch / n_words)\n",
    "        chainer.report({'perp': perp}, self)\n",
    "        return loss\n",
    "\n",
    "    def translate(self, xs, max_length=100):\n",
    "        batch = len(xs)\n",
    "        with chainer.no_backprop_mode(), chainer.using_config('train', False):\n",
    "            xs = [x[::-1] for x in xs]\n",
    "            exs = sequence_embed(self.embed_x, xs)\n",
    "            h, c, _ = self.encoder(None, None, exs)\n",
    "            ys = self.xp.full(batch, EOS, numpy.int32)\n",
    "            result = []\n",
    "            for i in range(max_length):\n",
    "                eys = self.embed_y(ys)\n",
    "                eys = F.split_axis(eys, batch, 0)\n",
    "                h, c, ys = self.decoder(h, c, eys)\n",
    "                cys = F.concat(ys, axis=0)\n",
    "                wy = self.W(cys)\n",
    "                ys = self.xp.argmax(wy.array, axis=1).astype(numpy.int32)\n",
    "                result.append(ys)\n",
    "\n",
    "        # Using `xp.concatenate(...)` instead of `xp.stack(result)` here to\n",
    "        # support NumPy 1.9.\n",
    "        result = cuda.to_cpu(\n",
    "            self.xp.concatenate([self.xp.expand_dims(x, 0) for x in result]).T)\n",
    "\n",
    "        # Remove EOS taggs\n",
    "        outs = []\n",
    "        for y in result:\n",
    "            inds = numpy.argwhere(y == EOS)\n",
    "            if len(inds) > 0:\n",
    "                y = y[:inds[0, 0]]\n",
    "            outs.append(y)\n",
    "        return outs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "そのまえ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert(batch, device):\n",
    "    def to_device_batch(batch):\n",
    "        if device is None:\n",
    "            return batch\n",
    "        elif device < 0:\n",
    "            return [chainer.dataset.to_device(device, x) for x in batch]\n",
    "        else:\n",
    "            xp = cuda.cupy.get_array_module(*batch)\n",
    "            concat = xp.concatenate(batch, axis=0)\n",
    "            sections = numpy.cumsum([len(x)\n",
    "                                     for x in batch[:-1]], dtype=numpy.int32)\n",
    "            concat_dev = chainer.dataset.to_device(device, concat)\n",
    "            batch_dev = cuda.cupy.split(concat_dev, sections)\n",
    "            return batch_dev\n",
    "\n",
    "    return {'xs': to_device_batch([x for x, _ in batch]),\n",
    "            'ys': to_device_batch([y for _, y in batch])}\n",
    "\n",
    "\n",
    "class CalculateBleu(chainer.training.Extension):\n",
    "\n",
    "    trigger = 1, 'epoch'\n",
    "    priority = chainer.training.PRIORITY_WRITER\n",
    "    print('そのまえ')\n",
    "    def __init__(\n",
    "            self, model, test_data, key, batch=100, device=-1, max_length=100):\n",
    "        print('いにっと')\n",
    "        self.model = model\n",
    "        self.test_data = test_data\n",
    "        self.key = key\n",
    "        self.batch = batch\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def forward(self, trainer):\n",
    "        print('かくにん')###################\n",
    "        with chainer.no_backprop_mode():\n",
    "            references = []\n",
    "            hypotheses = []\n",
    "            for i in range(0, len(self.test_data), self.batch):\n",
    "                sources, targets = zip(*self.test_data[i:i + self.batch])\n",
    "                references.extend([[t.tolist()] for t in targets])\n",
    "\n",
    "                sources = [\n",
    "                    chainer.dataset.to_device(self.device, x) for x in sources]\n",
    "                ys = [y.tolist()\n",
    "                      for y in self.model.translate(sources, self.max_length)]\n",
    "                hypotheses.extend(ys)\n",
    "\n",
    "        bleu = bleu_score.corpus_bleu(\n",
    "            references, hypotheses,\n",
    "            smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "        chainer.report({self.key: bleu})\n",
    "        print('おわり')\n",
    "\n",
    "\n",
    "def count_lines(path):\n",
    "    with open(path) as f:\n",
    "        return sum([1 for _ in f])\n",
    "\n",
    "\n",
    "def load_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        # +2 for UNK and EOS\n",
    "        word_ids = {line.strip(): i + 2 for i, line in enumerate(f)}\n",
    "    word_ids['<UNK>'] = 0\n",
    "    word_ids['<EOS>'] = 1\n",
    "    return word_ids\n",
    "\n",
    "\n",
    "def load_data(vocabulary, path):\n",
    "    n_lines = count_lines(path)\n",
    "    bar = progressbar.ProgressBar()\n",
    "    data = []\n",
    "    print('loading...: %s' % path)\n",
    "    with open(path) as f:\n",
    "#        for line in bar(f, max_value=n_lines):\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            words = line.strip().split()\n",
    "            array = numpy.array([vocabulary.get(w, UNK)\n",
    "                                 for w in words], numpy.int32)\n",
    "            data.append(array)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data_using_dataset_api(\n",
    "        src_vocab, src_path, target_vocab, target_path, filter_func):\n",
    "\n",
    "    def _transform_line(vocabulary, line):\n",
    "        words = line.strip().split()\n",
    "        return numpy.array(\n",
    "            [vocabulary.get(w, UNK) for w in words], numpy.int32)\n",
    "\n",
    "    def _transform(example):\n",
    "        source, target = example\n",
    "        return (\n",
    "            _transform_line(src_vocab, source),\n",
    "            _transform_line(target_vocab, target)\n",
    "        )\n",
    "\n",
    "    return chainer.datasets.TransformDataset(\n",
    "        chainer.datasets.TextDataset(\n",
    "            [src_path, target_path],\n",
    "            encoding='utf-8',\n",
    "            filter_func=filter_func\n",
    "        ), _transform)\n",
    "\n",
    "\n",
    "def calculate_unknown_ratio(data):\n",
    "    unknown = sum((s == UNK).sum() for s in data)\n",
    "    total = sum(s.size for s in data)\n",
    "    return unknown / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--validation-source VALIDATION_SOURCE]\n",
      "                             [--validation-target VALIDATION_TARGET]\n",
      "                             [--batchsize BATCHSIZE] [--epoch EPOCH]\n",
      "                             [--gpu GPU] [--resume RESUME] [--save SAVE]\n",
      "                             [--unit UNIT] [--layer LAYER] [--use-dataset-api]\n",
      "                             [--min-source-sentence MIN_SOURCE_SENTENCE]\n",
      "                             [--max-source-sentence MAX_SOURCE_SENTENCE]\n",
      "                             [--min-target-sentence MIN_TARGET_SENTENCE]\n",
      "                             [--max-target-sentence MAX_TARGET_SENTENCE]\n",
      "                             [--log-interval LOG_INTERVAL]\n",
      "                             [--validation-interval VALIDATION_INTERVAL]\n",
      "                             [--out OUT]\n",
      "                             SOURCE TARGET SOURCE_VOCAB TARGET_VOCAB\n",
      "ipykernel_launcher.py: error: the following arguments are required: TARGET, SOURCE_VOCAB, TARGET_VOCAB\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ikuta/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Chainer example: seq2seq')\n",
    "parser.add_argument('SOURCE', help='source sentence list')\n",
    "parser.add_argument('TARGET', help='target sentence list')\n",
    "parser.add_argument('SOURCE_VOCAB', help='source vocabulary file')\n",
    "parser.add_argument('TARGET_VOCAB', help='target vocabulary file')\n",
    "parser.add_argument('--validation-source',\n",
    "                    help='source sentence list for validation')\n",
    "parser.add_argument('--validation-target',\n",
    "                    help='target sentence list for validation')\n",
    "#    parser.add_argument('--batchsize', '-b', type=int, default=64,\n",
    "parser.add_argument('--batchsize', '-b', type=int, default=32,##\n",
    "                    help='number of sentence pairs in each mini-batch')\n",
    "#    parser.add_argument('--epoch', '-e', type=int, default=20,\n",
    "parser.add_argument('--epoch', '-e', type=int, default=40,##\n",
    "                    help='number of sweeps over the dataset to train')\n",
    "parser.add_argument('--gpu', '-g', type=int, default=-1,\n",
    "                    help='GPU ID (negative value indicates CPU)')\n",
    "parser.add_argument('--resume', '-r', default='',\n",
    "                    help='resume the training from snapshot')\n",
    "parser.add_argument('--save', '-s', default='',\n",
    "                    help='save a snapshot of the training')\n",
    "#    parser.add_argument('--unit', '-u', type=int, default=1024,\n",
    "parser.add_argument('--unit', '-u', type=int, default=300,\n",
    "                    help='number of units')\n",
    "#    parser.add_argument('--layer', '-l', type=int, default=3,\n",
    "parser.add_argument('--layer', '-l', type=int, default=2,##\n",
    "                    help='number of layers')\n",
    "#    parser.add_argument('--use-dataset-api', default=False,\n",
    "parser.add_argument('--use-dataset-api', default=True,\n",
    "                    action='store_true',\n",
    "                    help='use TextDataset API to reduce CPU memory usage')\n",
    "parser.add_argument('--min-source-sentence', type=int, default=1,\n",
    "                    help='minimium length of source sentence')\n",
    "#    parser.add_argument('--max-source-sentence', type=int, default=50,\n",
    "parser.add_argument('--max-source-sentence', type=int, default=14,##\n",
    "                    help='maximum length of source sentence')\n",
    "parser.add_argument('--min-target-sentence', type=int, default=1,\n",
    "                    help='minimium length of target sentence')\n",
    "#    parser.add_argument('--max-target-sentence', type=int, default=50,\n",
    "parser.add_argument('--max-target-sentence', type=int, default=14,##\n",
    "                    help='maximum length of target sentence')\n",
    "parser.add_argument('--log-interval', type=int, default=200,\n",
    "                    help='number of iteration to show log')\n",
    "parser.add_argument('--validation-interval', type=int, default=4000,\n",
    "                    help='number of iteration to evlauate the model '\n",
    "                    'with validation dataset')\n",
    "parser.add_argument('--out', '-o', default='result',\n",
    "                    help='directory to output the result')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_VOCAB = 'vocab_orisim.txt'\n",
    "TARGET_VOCAB = 'vocab_orisim.txt'\n",
    "SOURCE='original_train_prepro2.txt'\n",
    "TARGET='simple_train_prepro2.txt'\n",
    "validation_source='original_test_prepro.txt'\n",
    "validation_target='simple_test_prepro.txt'\n",
    "batchsize=32\n",
    "epoch = 40\n",
    "gpu=0\n",
    "resume=''\n",
    "save=''\n",
    "unit=300\n",
    "layer=2\n",
    "use_dataset_api=True\n",
    "min_source_sentence=1\n",
    "max_source_sentence=14\n",
    "min_target_sentence=1\n",
    "max_target_sentence=14\n",
    "log_interval=200\n",
    "validation_interval=4000\n",
    "out='result'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-12-20 18:35:52.333706] Loading dataset... (this may take several minutes)\n",
      "10942\n",
      "[2018-12-20 18:35:54.596024] Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-processed dataset\n",
    "print('[{}] Loading dataset... (this may take several minutes)'.format(\n",
    "    datetime.datetime.now()))\n",
    "source_ids = load_vocabulary(SOURCE_VOCAB)\n",
    "target_ids = load_vocabulary(TARGET_VOCAB)\n",
    "print (len(source_ids))\n",
    "\n",
    "if use_dataset_api:\n",
    "    # By using TextDataset, you can avoid loading whole dataset on memory.\n",
    "    # This significantly reduces the host memory usage.\n",
    "    def _filter_func(s, t):\n",
    "        sl = len(s.strip().split())  # number of words in source line\n",
    "        tl = len(t.strip().split())  # number of words in target line\n",
    "        return (\n",
    "            min_source_sentence <= sl <= max_source_sentence and\n",
    "            min_target_sentence <= tl <= max_target_sentence)\n",
    "\n",
    "    train_data = load_data_using_dataset_api(\n",
    "        source_ids, SOURCE,\n",
    "        target_ids, TARGET,\n",
    "        _filter_func,\n",
    "    )\n",
    "else:\n",
    "    # Load all records on memory.\n",
    "    train_source = load_data(source_ids, SOURCE)\n",
    "    train_target = load_data(target_ids, TARGET)\n",
    "    assert len(train_source) == len(train_target)\n",
    "\n",
    "    train_data = [\n",
    "        (s, t)\n",
    "        for s, t in six.moves.zip(train_source, train_target)\n",
    "        if (min_source_sentence <= len(s) <= max_source_sentence\n",
    "            and\n",
    "            min_target_sentence <= len(t) <= max_target_sentence)\n",
    "    ]\n",
    "print('[{}] Dataset loaded.'.format(datetime.datetime.now()))\n",
    "\n",
    "if not use_dataset_api:\n",
    "    # Skip printing statistics when using TextDataset API, as it is slow.\n",
    "    train_source_unknown = calculate_unknown_ratio(\n",
    "        [s for s, _ in train_data])\n",
    "    train_target_unknown = calculate_unknown_ratio(\n",
    "        [t for _, t in train_data])\n",
    "\n",
    "    print('Source vocabulary size: %d' % len(source_ids))\n",
    "    print('Target vocabulary size: %d' % len(target_ids))\n",
    "    print('Train data size: %d' % len(train_data))\n",
    "    print('Train source unknown ratio: %.2f%%' % (\n",
    "        train_source_unknown * 100))\n",
    "    print('Train target unknown ratio: %.2f%%' % (\n",
    "        train_target_unknown * 100))\n",
    "\n",
    "target_words = {i: w for w, i in target_ids.items()}\n",
    "source_words = {i: w for w, i in source_ids.items()}\n",
    "\n",
    "# Setup model\n",
    "model = Seq2seq(layer, len(source_ids), len(target_ids), unit)\n",
    "if gpu >= 0:\n",
    "    chainer.backends.cuda.get_device(gpu).use()\n",
    "    model.to_gpu(gpu)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)\n",
    "\n",
    "# Setup iterator\n",
    "train_iter = chainer.iterators.SerialIterator(train_data, batchsize)\n",
    "\n",
    "# Setup updater and trainer\n",
    "updater = training.updaters.StandardUpdater(\n",
    "    train_iter, optimizer, converter=convert, device=gpu)\n",
    "trainer = training.Trainer(updater, (epoch, 'epoch'), out=out)\n",
    "trainer.extend(extensions.LogReport(\n",
    "    trigger=(log_interval, 'iteration')))\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch', 'iteration', 'main/loss', 'validation/main/loss',\n",
    "     'main/perp', 'validation/main/perp', 'validation/main/bleu',\n",
    "     'elapsed_time']),\n",
    "    trigger=(log_interval, 'iteration'))\n",
    "trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print (type(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "batch1 = batch[0]\n",
    "print (type(batch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print (len(batch1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_data, _label = batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80, 218,   5, 147,   7,  25,  48,  15,  13], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80, 218,   5, 147,   7,  25,  48,  15,  13], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if validation_source and validation_target:\n",
    "    if use_dataset_api: ##\n",
    "        # By using TextDataset, you can avoid loading whole dataset on memory. ##\n",
    "        # This significantly reduces the host memory usage. ##\n",
    "        def _filter_func(s, t):##\n",
    "            sl = len(s.strip().split())  # number of words in source line ##\n",
    "            tl = len(t.strip().split())  # number of words in target line ##\n",
    "            return (##\n",
    "    #                    args.min_source_sentence <= sl <= args.max_source_sentence and ##\n",
    "    #                    args.min_target_sentence <= tl <= args.max_target_sentence ##\n",
    "                0 < len(s) and ##\n",
    "                0 < len(t) ) ##\n",
    "        test_data = load_data_using_dataset_api( ##\n",
    "            source_ids, validation_source, ##\n",
    "            target_ids, validation_target, ##\n",
    "            _filter_func,##\n",
    "        ) ##\n",
    "    else:\n",
    "        test_source = load_data(source_ids, validation_source)\n",
    "        test_target = load_data(target_ids, validation_target)\n",
    "        assert len(test_source) == len(test_target)\n",
    "        test_data = list(six.moves.zip(test_source, test_target))\n",
    "        test_data = [(s, t) for s, t in test_data if 0 < len(s) and 0 < len(t)]\n",
    "        test_source_unknown = calculate_unknown_ratio(\n",
    "            [s for s, _ in test_data])\n",
    "        test_target_unknown = calculate_unknown_ratio(\n",
    "            [t for _, t in test_data])\n",
    "\n",
    "        print('Validation data: %d' % len(test_data))\n",
    "        print('Validation source unknown ratio: %.2f%%' %\n",
    "          (test_source_unknown * 100))\n",
    "        print('Validation target unknown ratio: %.2f%%' %\n",
    "          (test_target_unknown * 100))\n",
    "\n",
    "test_iter = chainer.iterators.SerialIterator(test_data, batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bt = next(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bt1 = test_bt[0]\n",
    "type(test_bt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_bt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_data, _test_label = test_bt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "key= 'validation/main/bleu'\n",
    "batch=100\n",
    "device=0\n",
    "max_length=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "range() arg 3 must not be zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-fa71fff8d4ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m trainer.extend(\n\u001b[1;32m     40\u001b[0m     CalculateBleu(\n\u001b[0;32m---> 41\u001b[0;31m         model, test_data, 'validation/main/bleu', gpu),\n\u001b[0m\u001b[1;32m     42\u001b[0m     trigger=(validation_interval, 'iteration'))\n",
      "\u001b[0;32m<ipython-input-32-fa71fff8d4ae>\u001b[0m in \u001b[0;36mCalculateBleu\u001b[0;34m(model, test_data, key, batch, device, max_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mhypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: range() arg 3 must not be zero"
     ]
    }
   ],
   "source": [
    "        @chainer.training.make_extension()\n",
    "        def translate(trainer):\n",
    "            source, target = test_data[numpy.random.choice(len(test_data))]\n",
    "            result = model.translate([model.xp.array(source)])[0]\n",
    "\n",
    "            source_sentence = ' '.join([source_words[x] for x in source])\n",
    "            target_sentence = ' '.join([target_words[y] for y in target])\n",
    "            result_sentence = ' '.join([target_words[y] for y in result])\n",
    "            print('# source : ' + source_sentence)\n",
    "            print('# result : ' + result_sentence)\n",
    "            print('# expect : ' + target_sentence)\n",
    "\n",
    "        @chainer.training.make_extension()\n",
    "        def CalculateBleu(model, test_data, key, batch=100, device=-1, max_length=100):\n",
    "            trigger = 1, 'epoch'\n",
    "            priority = chainer.training.PRIORITY_WRITER\n",
    "            with chainer.no_backprop_mode():\n",
    "                references = []\n",
    "                hypotheses = []\n",
    "                for i in range(0, len(test_data), batch):\n",
    "                    sources, targets = zip(*test_data[i:i + batch])\n",
    "                    references.extend([[t.tolist()] for t in targets])\n",
    "\n",
    "                    sources = [\n",
    "                    chainer.dataset.to_device(device, x) for x in sources]\n",
    "                    ys = [y.tolist()\n",
    "                      for y in model.translate(sources, max_length)]\n",
    "                    hypotheses.extend(ys)\n",
    "\n",
    "            bleu = bleu_score.corpus_bleu(\n",
    "                references, hypotheses,\n",
    "                smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "            chainer.report({key: bleu})\n",
    "     \n",
    "            \n",
    "        trainer.extend(\n",
    "            translate, trigger=(validation_interval, 'iteration'))\n",
    "\n",
    "        trainer.extend(\n",
    "            CalculateBleu(\n",
    "                model, test_data, 'validation/main/bleu', gpu),\n",
    "            trigger=(validation_interval, 'iteration'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "    source_ids = load_vocabulary(SOURCE_VOCAB)\n",
    "    target_ids = load_vocabulary(TARGET_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def _filter_func(s, t):##\n",
    "        sl = len(s.strip().split())  # number of words in source line ##\n",
    "        tl = len(t.strip().split())  # number of words in target line ##\n",
    "        return (\n",
    "            0 < sl and ##\n",
    "            0 < tl ) ##\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49418\n"
     ]
    }
   ],
   "source": [
    "    test_data = load_data_using_dataset_api( ##\n",
    "        source_ids, SOURCE, ##\n",
    "        target_ids, TARGET, ##\n",
    "        _filter_func,##\n",
    "    ) ##\n",
    "    print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "    target_words = {i: w for w, i in target_ids.items()}\n",
    "    source_words = {i: w for w, i in source_ids.items()}\n",
    "\n",
    "    # Setup model\n",
    "    model = Seq2seq(layer, len(source_ids), len(target_ids), unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "    resume = 'sna'\n",
    "    chainer.serializers.load_npz(resume, model)\n",
    "    \n",
    "    if gpu >= 0:\n",
    "        chainer.backends.cuda.get_device(gpu).use()\n",
    "        model.to_gpu(gpu)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def translate(test_data):\n",
    "        source, target = test_data[numpy.random.choice(len(test_data))]\n",
    "        result = model.translate([model.xp.array(source)])[0]\n",
    "        \n",
    "        source_sentence = ' '.join([source_words[x] for x in source])\n",
    "        target_sentence = ' '.join([target_words[y] for y in target])\n",
    "        result_sentence = ' '.join([target_words[y] for y in result])\n",
    "        print('# source : ' + source_sentence)\n",
    "        print('# result : ' + result_sentence)\n",
    "        print('# expect : ' + target_sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def CalculateBleu(model=model, test_data=test_data, key='validation/main/bleu', batch = 100, max_length=100, device=gpu):            \n",
    "        trigger = 1, 'epoch'\n",
    "        priority = chainer.training.PRIORITY_WRITER\n",
    "        with chainer.no_backprop_mode():\n",
    "            references = []\n",
    "            hypotheses = []\n",
    "            for i in range(0, len(test_data), batch):\n",
    "                sources, targets = zip(*test_data[i:i + batch])\n",
    "                references.extend([[t.tolist()] for t in targets])\n",
    "                sources = [\n",
    "                    chainer.dataset.to_device(device, x) for x in sources]\n",
    "                ys = [y.tolist()\n",
    "                      for y in model.translate(sources, max_length)]\n",
    "                hypotheses.extend(ys)\n",
    "            print(len(reference))\n",
    "            bleu = bleu_score.corpus_bleu(\n",
    "                references, hypotheses,\n",
    "                smoothing_function=bleu_score.SmoothingFunction().method1)\n",
    "            chainer.report({key: bleu})\n",
    "            print(bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    CalculateBleu(model=model, test_data=test_data, key='validation/main/bleu', batch = 100, max_length=100, device=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
